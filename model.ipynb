{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from statsmodels.formula.api import ols\n",
    "import sklearn as sk \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Global_INT.xlsx\", sheet_name=\"Global-INT\")\n",
    "dicti = [\"continent_code\",\t\"country_code\",\t\"age\",\t\"sex\",\t\"gender_identity\",\t\"sexual_orientation\",\t\"height\",\t\"weight\",\t\"bmi\",\t\"marital_status\",\t\n",
    "         \"student_accommodation\",\t\"work\",\t\"income_reportada_corrigida\",\t\"income_categorias de renda pelo salário mínimo nacional_CLASSIFICAÇÃO\",\t\n",
    "         \"income_razão da renda pelo salário mínimo nacional\",\t\"income_grupos de referência pelo percentil_20,40,60,80,100\",\t\"score_food_smile\",\t\n",
    "         \"score_subs_smile\",\t\"score_Physical Activity_smile\",\t\"score_stress_smile\",\t\"score_social_smile\",\t\"score_sleep_smile\",\t\"score_envir_smile\",\t\n",
    "         \"score_total_smile\",\t\"sedentary_behavior\",\t'sedentary_2',\t\"gad7_score\",\t\"gad7_severidade de sintomas\"]\n",
    "\n",
    "cat_var = [\"continent_code\", \"country_code\", \"sex\",\t\"gender_identity\",\t\"sexual_orientation\", \"marital_status\",\t\n",
    "           \"student_accommodation\",\t\"work\", \"income_grupos de referência pelo percentil_20,40,60,80,100\", \"gad7_severidade de sintomas\",\n",
    "           \"sedentary_behavior\",\t'sedentary_2',]\n",
    "\n",
    "num_var = [\"age\", \"bmi\", \"score_food_smile\", \"score_subs_smile\",\t\"score_Physical Activity_smile\",\t\"score_stress_smile\",\t\n",
    "           \"score_social_smile\",\t\"score_sleep_smile\",\t\"score_envir_smile\",\t\n",
    "           \"score_total_smile\", \"gad7_score\"]\n",
    "\n",
    "df.drop(columns=[\"height\", \"weight\", \"income_reportada_corrigida\",\t\n",
    "                 \"income_categorias de renda pelo salário mínimo nacional_CLASSIFICAÇÃO\",\t\n",
    "                 \"income_razão da renda pelo salário mínimo nacional\",\"income_reportada_corrigida\",\t\n",
    "                 \"income_categorias de renda pelo salário mínimo nacional_CLASSIFICAÇÃO\"],\n",
    "                 inplace = True)\n",
    "\n",
    "for column in cat_var: \n",
    "    if not column == \"gad7_severidade de sintomas\": df[column] = df[column]-1\n",
    "\n",
    "df.to_excel(\"Global_INT_15_02.xlsx\", index = False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando os continentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#       separei os continentes em planilhas diferentes porque acho que assim conseguiria compartimentalizar\n",
    "#       o código de forma melhor. também estou mais acostumado a trabalhar com as planilhas assim, já que\n",
    "#       foi dessa forma que as utilizei na Fase 1\n",
    "df = pd.read_excel(\"Global_INT_15_02.xlsx\")\n",
    "\n",
    "print(\"Columns found:\", df.columns.tolist())\n",
    "print(\"Data preview:\\n\", df.head())\n",
    "\n",
    "if \"continent_code\" not in df.columns:\n",
    "    raise ValueError(\"The column 'continent_code' was not found in the file. Please check the header name.\")\n",
    "\n",
    "try:\n",
    "    df[\"continent_code\"] = df[\"continent_code\"].astype(int)\n",
    "except Exception as e:\n",
    "    print(\"Could not convert 'continent_code' to int:\", e)\n",
    "\n",
    "continent_mapping = {\n",
    "    0: \"América do Sul\",\n",
    "    1: \"América do Norte\",\n",
    "    2: \"Oceania\",\n",
    "    3: \"Ásia\",\n",
    "    4: \"África\",\n",
    "    5: \"Europa\"\n",
    "}\n",
    "\n",
    "for code, continent in continent_mapping.items():\n",
    "    subset = df[df[\"continent_code\"] == code]\n",
    "    output_filename = f\"{continent}_MODEL.xlsx\"\n",
    "    subset.to_excel(output_filename, index=False)\n",
    "    print(f\"Created {output_filename} with {len(subset)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição das variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#AMÉRICA DO SUL\n",
    "\n",
    "data = pd.read_excel(\"América do Sul_MODEL.xlsx\", sheet_name='Sheet1')\n",
    "\n",
    "cat_col_minus_oh = [\"sex\", \"student_accommodation\", \"work\", \n",
    "                    \"income_grupos de referência pelo percentil_20,40,60,80,100\",\n",
    "                    \"sedentary_behavior\", 'sedentary_2']\n",
    "\n",
    "one_hotted = [\"marital_status\", \"gender_identity\", \"sexual_orientation\", \"country_code\"]                \n",
    "target_col = \"gad7_severidade de sintomas\"\n",
    "\n",
    "data = data.drop(columns=[\"continent_code\", \"gad7_score\"])\n",
    "\n",
    "X = data.drop(columns=[target_col])\n",
    "Y = data[target_col]\n",
    "\n",
    "missing_data_idx = X[X.isna().any(axis=1)].index.to_numpy()\n",
    "complete_data_idx = X.dropna().index.to_numpy()\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(\"América do Suil_MODEL_treated.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essa célula serve pra expandir as colunas/linhas do df.head(), assim consigo ver todo o split e entender melhor como ele tá separando\n",
    "# pra voltar pra visualização padrão, só aplico o reset nele\n",
    "\n",
    "#pd.set_option(\"display.max_rows\", None)  \n",
    "#pd.set_option(\"display.max_columns\", None)  \n",
    "#pd.set_option(\"display.width\", None) \n",
    "pd.reset_option(\"display.max_rows\")\n",
    "pd.reset_option(\"display.max_columns\")\n",
    "pd.reset_option(\"display.width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratando os splits - separando, codificando e escalonando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: (6167, 37), Teste: (1210, 37)\n",
      "       marital_status_0.0  marital_status_1.0  marital_status_2.0  \\\n",
      "count         6167.000000         6167.000000         6167.000000   \n",
      "mean             0.013783            0.951516            0.000324   \n",
      "std              0.116599            0.214804            0.018007   \n",
      "min              0.000000            0.000000            0.000000   \n",
      "25%              0.000000            1.000000            0.000000   \n",
      "50%              0.000000            1.000000            0.000000   \n",
      "75%              0.000000            1.000000            0.000000   \n",
      "max              1.000000            1.000000            1.000000   \n",
      "\n",
      "       marital_status_3.0  marital_status_4.0  marital_status_nan  \\\n",
      "count         6167.000000         6167.000000         6167.000000   \n",
      "mean             0.001784            0.029512            0.003081   \n",
      "std              0.042199            0.169250            0.055425   \n",
      "min              0.000000            0.000000            0.000000   \n",
      "25%              0.000000            0.000000            0.000000   \n",
      "50%              0.000000            0.000000            0.000000   \n",
      "75%              0.000000            0.000000            0.000000   \n",
      "max              1.000000            1.000000            1.000000   \n",
      "\n",
      "       gender_identity_0.0  gender_identity_1.0  gender_identity_nan  \\\n",
      "count          6167.000000          6167.000000          6167.000000   \n",
      "mean              0.955732             0.034701             0.009567   \n",
      "std               0.205706             0.183036             0.097350   \n",
      "min               0.000000             0.000000             0.000000   \n",
      "25%               1.000000             0.000000             0.000000   \n",
      "50%               1.000000             0.000000             0.000000   \n",
      "75%               1.000000             0.000000             0.000000   \n",
      "max               1.000000             1.000000             1.000000   \n",
      "\n",
      "       sexual_orientation_0.0  ...  score_social_smile  score_sleep_smile  \\\n",
      "count             6167.000000  ...         6119.000000        6132.000000   \n",
      "mean                 0.691584  ...            0.154341          -0.230344   \n",
      "std                  0.461877  ...            0.566781           0.542677   \n",
      "min                  0.000000  ...           -1.000000          -1.000000   \n",
      "25%                  0.000000  ...           -0.294118          -0.647059   \n",
      "50%                  1.000000  ...            0.294118          -0.411765   \n",
      "75%                  1.000000  ...            0.647059           0.058824   \n",
      "max                  1.000000  ...            1.000000           1.000000   \n",
      "\n",
      "       score_envir_smile  score_total_smile          sex  \\\n",
      "count        6132.000000        5904.000000  6156.000000   \n",
      "mean           -0.232632          -0.068198     0.395062   \n",
      "std             0.348461           0.290413     0.488904   \n",
      "min            -1.000000          -1.000000     0.000000   \n",
      "25%            -0.500000          -0.283019     0.000000   \n",
      "50%            -0.166667          -0.056604     0.000000   \n",
      "75%             0.000000           0.132075     1.000000   \n",
      "max             1.000000           1.000000     1.000000   \n",
      "\n",
      "       student_accommodation         work  sedentary_behavior  sedentary_2  \\\n",
      "count            6154.000000  6143.000000         6160.000000  6163.000000   \n",
      "mean                0.979688     0.811818            2.033117     1.117962   \n",
      "std                 0.141077     0.390889            1.013727     1.076982   \n",
      "min                 0.000000     0.000000            0.000000     0.000000   \n",
      "25%                 1.000000     1.000000            1.000000     0.000000   \n",
      "50%                 1.000000     1.000000            2.000000     1.000000   \n",
      "75%                 1.000000     1.000000            3.000000     2.000000   \n",
      "max                 1.000000     1.000000            4.000000     4.000000   \n",
      "\n",
      "       income_grupos de referência pelo percentil_20,40,60,80,100  \n",
      "count                                        5412.000000           \n",
      "mean                                            1.961567           \n",
      "std                                             1.421253           \n",
      "min                                             0.000000           \n",
      "25%                                             1.000000           \n",
      "50%                                             2.000000           \n",
      "75%                                             3.000000           \n",
      "max                                             4.000000           \n",
      "\n",
      "[8 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# separando os splits treino/teste (80/20)\n",
    "shuffle_split = ShuffleSplit(n_splits=5, test_size=0.2, random_state=50)\n",
    "splits = []\n",
    "\n",
    "one_hotted = [\"marital_status\", \"gender_identity\", \"sexual_orientation\", \"country_code\"]  \n",
    "\n",
    "cat_noh = [ 'sex', 'student_accommodation', 'work','sedentary_behavior','sedentary_2', \n",
    "           'income_grupos de referência pelo percentil_20,40,60,80,100']\n",
    "\n",
    "num = ['age', 'bmi', 'score_food_smile', 'score_subs_smile', 'score_Physical Activity_smile',\n",
    "       'score_stress_smile', 'score_social_smile', 'score_sleep_smile',\n",
    "       'score_envir_smile', 'score_total_smile']\n",
    "\n",
    "for train_pos, test_pos in shuffle_split.split(complete_data_idx):\n",
    "    train_idx = complete_data_idx[train_pos]\n",
    "    test_idx = complete_data_idx[test_pos]\n",
    "\n",
    "    train_idx = pd.Index(train_idx).union(missing_data_idx)  \n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "\n",
    "\n",
    "    # One-hot encoding e StandartScaler\n",
    "    onehot_transformer = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    scaler_transformer = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"onehot\", onehot_transformer, one_hotted),\n",
    "            (\"scaler\", scaler_transformer, num),\n",
    "            (\"pass\", \"passthrough\", cat_noh)\n",
    "        ],\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    #  codficando e escalonando\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Get one-hot feature names from the onehot transformer\n",
    "    onehot_names = preprocessor.named_transformers_[\"onehot\"].get_feature_names_out(one_hotted)\n",
    "\n",
    "    scaled_names = num\n",
    "\n",
    "    pass_names = cat_noh\n",
    "\n",
    "    remaining = [col for col in X_train.columns if col not in one_hotted + num + cat_noh]\n",
    "\n",
    "    # Combine all the names in the correct order:\n",
    "    all_feature_names = np.concatenate([onehot_names, scaled_names, pass_names, remaining])\n",
    "\n",
    "    X_train = pd.DataFrame(X_train_transformed, index=X_train.index, columns=all_feature_names)\n",
    "    X_test = pd.DataFrame(X_test_transformed, index=X_test.index, columns=all_feature_names)\n",
    "\n",
    "    # guardando splits com os dados transformados\n",
    "    splits.append({\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"Y_train\": Y_train,\n",
    "        \"Y_test\": Y_test\n",
    "    })\n",
    "\n",
    "# debugging: check do \"transformer\"\n",
    "print(f\"Treino: {splits[0]['X_train'].shape}, Teste: {splits[0]['X_test'].shape}\")\n",
    "print(splits[4]['X_train'].describe())  # Inspect transformed values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando a imputação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#após separados, escalonados e codificados, aplicar inputação de dados faltantes\n",
    "#método k-Nearest Neighbour - nota: tentei aumentar e diminuir o n de vizinhos; pouca alteração\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "for split in splits: #iterando pelos splits\n",
    "\n",
    "    X_train_inputed = pd.DataFrame(knn_imputer.fit_transform(split[\"X_train\"]), \n",
    "                                   columns=split[\"X_train\"].columns)\n",
    "    \n",
    "    #print(\"Depois da inputação:\")\n",
    "    #print(f\"Valores faltantes no treino - split:\", X_train_inputed.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testeando a similaridade dos dados inputados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variável bmi  NÃO tem distribuições similares (failed null-hypothesis) - 0.031890807211219965\n",
      "Variável income_grupos de referência pelo percentil_20,40,60,80,100  NÃO tem distribuições similares (failed null-hypothesis) - 0.001562125870921987\n",
      "Variável bmi  NÃO tem distribuições similares (failed null-hypothesis) - 7.050327269234771e-51\n",
      "Variável income_grupos de referência pelo percentil_20,40,60,80,100  NÃO tem distribuições similares (failed null-hypothesis) - 0.000569485469545863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvcha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\stats\\_axis_nan_policy.py:573: RuntimeWarning: ks_2samp: Exact calculation unsuccessful. Switching to method=asymp.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variável score_total_smile  NÃO tem distribuições similares (failed null-hypothesis) - 4.9155704005691015e-102\n",
      "Variável income_grupos de referência pelo percentil_20,40,60,80,100  NÃO tem distribuições similares (failed null-hypothesis) - 0.0013435097865340821\n",
      "Variável bmi  NÃO tem distribuições similares (failed null-hypothesis) - 0.004709067703457136\n",
      "Variável score_total_smile  NÃO tem distribuições similares (failed null-hypothesis) - 2.674471458703257e-99\n",
      "Variável income_grupos de referência pelo percentil_20,40,60,80,100  NÃO tem distribuições similares (failed null-hypothesis) - 0.0011532718955703373\n",
      "Variável income_grupos de referência pelo percentil_20,40,60,80,100  NÃO tem distribuições similares (failed null-hypothesis) - 0.0008462248555998776\n"
     ]
    }
   ],
   "source": [
    "c2 = ['marital_status_0.0', 'marital_status_1.0', 'marital_status_2.0',\n",
    "       'marital_status_3.0', 'marital_status_4.0', 'marital_status_nan',\n",
    "       'gender_identity_0.0', 'gender_identity_1.0', 'gender_identity_nan',\n",
    "       'sexual_orientation_0.0', 'sexual_orientation_1.0',\n",
    "       'sexual_orientation_2.0', 'sexual_orientation_3.0',\n",
    "       'sexual_orientation_4.0', 'sexual_orientation_nan', 'country_code_0',\n",
    "       'country_code_1', 'country_code_2', 'country_code_3', 'country_code_4',\n",
    "       'country_code_5', 'sex', 'student_accommodation', 'work',\n",
    "       'sedentary_behavior','sedentary_2'] \n",
    "\n",
    "ks = ['age', 'bmi', 'income_grupos de referência pelo percentil_20,40,60,80,100', \n",
    "      'score_food_smile', 'score_subs_smile', 'score_Physical Activity_smile',\n",
    "      'score_stress_smile', 'score_social_smile', 'score_sleep_smile',\n",
    "      'score_envir_smile', 'score_total_smile']\n",
    "\n",
    "for split in splits:\n",
    "    #testando a similaridade dos dados inputados vs originais\n",
    "    #teste KS para variáveis numéricas\n",
    "    for col in split[\"X_train\"].columns:\n",
    "        if col in ks:\n",
    "            original_values = split[\"X_train\"][col].dropna()\n",
    "            inputed_values = X_train_inputed[col]\n",
    "\n",
    "            ks_stat, p_value = ks_2samp(original_values, inputed_values)\n",
    "\n",
    "            if p_value < 0.05: print(f\"Variável {col}  NÃO tem distribuições similares (failed null-hypothesis) - {p_value}\")\n",
    "            #else: print(f\"Variável {col} NÃO tem distribuições similares (proved null-hypothesis) - {p_value}\")\n",
    "\n",
    "    #chi-quadrado para testar variação entre categóricas\n",
    "    for col in c2:\n",
    "        original_counts = split[\"X_train\"][col].value_counts()\n",
    "        inputed_counts = X_train_inputed[col].round().astype(int).value_counts()\n",
    "\n",
    "        original_counts= original_counts.reindex(inputed_counts.index, fill_value=0)\n",
    "        inputed_counts= inputed_counts.reindex(original_counts.index, fill_value=0)\n",
    "\n",
    "        chi2, p_value, _, _ = chi2_contingency([original_counts, inputed_counts])\n",
    "\n",
    "        if p_value < 0.05: print(f\"Variável {col} AFETADA pela inputação (failed null-hypothesis) - {p_value}\")\n",
    "        #else: print(f\"Variável {col} AFETADA pela inputação (proved null-hypothesis) - {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse output, tem um erro que não sei explicar o que é exatamente; pesquisei e parece ser só uma \"mudança na forma de cálculo\" do teste KS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
