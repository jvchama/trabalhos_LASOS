{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from statsmodels.formula.api import ols\n",
    "import sklearn as sk \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Global_INT.xlsx\", sheet_name=\"Global-INT\")\n",
    "dicti = [\"continent_code\",\t\"country_code\",\t\"age\",\t\"sex\",\t\"gender_identity\",\t\"sexual_orientation\",\t\"height\",\t\"weight\",\t\"bmi\",\t\"marital_status\",\t\n",
    "         \"student_accommodation\",\t\"work\",\t\"income_reportada_corrigida\",\t\"income_categorias de renda pelo salário mínimo nacional_CLASSIFICAÇÃO\",\t\n",
    "         \"income_razão da renda pelo salário mínimo nacional\",\t\"income_grupos de referência pelo percentil_20,40,60,80,100\",\t\"score_food_smile\",\t\n",
    "         \"score_subs_smile\",\t\"score_Physical Activity_smile\",\t\"score_stress_smile\",\t\"score_social_smile\",\t\"score_sleep_smile\",\t\"score_envir_smile\",\t\n",
    "         \"score_total_smile\",\t\"sedentary_behavior\",\t'sedentary_2',\t\"gad7_score\",\t\"gad7_severidade de sintomas\"]\n",
    "\n",
    "cat_var = [\"continent_code\", \"country_code\", \"sex\",\t\"gender_identity\",\t\"sexual_orientation\", \"marital_status\",\t\n",
    "           \"student_accommodation\",\t\"work\", \"income_grupos de referência pelo percentil_20,40,60,80,100\", \"gad7_severidade de sintomas\",\n",
    "           \"sedentary_behavior\",\t'sedentary_2',]\n",
    "\n",
    "num_var = [\"age\", \"bmi\", \"score_food_smile\", \"score_subs_smile\",\t\"score_Physical Activity_smile\",\t\"score_stress_smile\",\t\n",
    "           \"score_social_smile\",\t\"score_sleep_smile\",\t\"score_envir_smile\",\t\n",
    "           \"score_total_smile\", \"gad7_score\"]\n",
    "\n",
    "df.drop(columns=[\"height\", \"weight\", \"income_reportada_corrigida\",\t\n",
    "                 \"income_categorias de renda pelo salário mínimo nacional_CLASSIFICAÇÃO\",\t\n",
    "                 \"income_razão da renda pelo salário mínimo nacional\",\"income_reportada_corrigida\",\t\n",
    "                 \"income_categorias de renda pelo salário mínimo nacional_CLASSIFICAÇÃO\"],\n",
    "                 inplace = True)\n",
    "\n",
    "for column in cat_var: \n",
    "    if not column == \"gad7_severidade de sintomas\": df[column] = df[column]-1\n",
    "\n",
    "df.to_excel(\"Global_INT_15_02.xlsx\", index = False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Global_INT_15_02.xlsx\")\n",
    "\n",
    "print(\"Columns found:\", df.columns.tolist())\n",
    "print(\"Data preview:\\n\", df.head())\n",
    "\n",
    "if \"continent_code\" not in df.columns:\n",
    "    raise ValueError(\"The column 'continent_code' was not found in the file. Please check the header name.\")\n",
    "\n",
    "try:\n",
    "    df[\"continent_code\"] = df[\"continent_code\"].astype(int)\n",
    "except Exception as e:\n",
    "    print(\"Could not convert 'continent_code' to int:\", e)\n",
    "\n",
    "continent_mapping = {\n",
    "    0: \"América do Sul\",\n",
    "    1: \"América do Norte\",\n",
    "    2: \"Oceania\",\n",
    "    3: \"Ásia\",\n",
    "    4: \"África\",\n",
    "    5: \"Europa\"\n",
    "}\n",
    "\n",
    "for code, continent in continent_mapping.items():\n",
    "    subset = df[df[\"continent_code\"] == code]\n",
    "    output_filename = f\"{continent}_MODEL.xlsx\"\n",
    "    subset.to_excel(output_filename, index=False)\n",
    "    print(f\"Created {output_filename} with {len(subset)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data = pd.read_excel(\"Global_INT_15_02.xlsx\", sheet_name='Sheet1')\n",
    "df = pd.DataFrame(data)\n",
    "one_hotted = [\"marital_status\", \"gender_identity\", \"sexual_orientation\"]\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\") # ***ignorando dados faltantes nas one_hot***\n",
    "encoded = encoder.fit_transform(df[one_hotted])\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(one_hotted))\n",
    "df = pd.concat([df.drop(columns = one_hotted), encoded_df], axis=1)\n",
    "df.to_excel(\"Global_INT_One_Hot.xlsx\", index=False)\n",
    "#print(df.head())\n",
    "\n",
    "#36 colunas no total pós one_hot vs 23 pré one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#AMÉRICA DO SUL\n",
    "\n",
    "data = pd.read_excel(\"América do Sul_MODEL.xlsx\", sheet_name='Sheet1')\n",
    "\n",
    "cat_col_minus_oh = [\"country_code\", \"sex\", \"student_accommodation\", \"work\", \n",
    "                    \"income_grupos de referência pelo percentil_20,40,60,80,100\",\n",
    "                    \"sedentary_behavior\", 'sedentary_2']\n",
    "\n",
    "one_hotted = [\"marital_status\", \"gender_identity\", \"sexual_orientation\"]                \n",
    "target_col = \"gad7_severidade de sintomas\"\n",
    "\n",
    "X = data.drop(columns=[target_col])\n",
    "Y = data[target_col]\n",
    "         \n",
    "missing_data_idx = X[X.isna().any(axis=1)].index.to_numpy()\n",
    "complete_data_idx = X.dropna().index.to_numpy()\n",
    "\n",
    "shuffle_split = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "splits = []\n",
    "\n",
    "for train_pos, test_pos in shuffle_split.split(complete_data_idx):\n",
    "\n",
    "    train_idx = complete_data_idx[train_pos]  \n",
    "    test_idx = complete_data_idx[test_pos]\n",
    "\n",
    "    train_idx = pd.Index(train_idx).union(missing_data_idx)          \n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "\n",
    "    # guardando split como dictionary\n",
    "    splits.append({\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"Y_train\": Y_train,\n",
    "        \"Y_test\": Y_test\n",
    "        })\n",
    "    \n",
    "    onehot_transformer = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    scaler_transformer = StandardScaler()\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        (\"onehot\", onehot_transformer, one_hotted), \n",
    "        (\"scaler\", scaler_transformer, cat_col_minus_oh + num_var)  \n",
    "                     ],\n",
    "    remainder='passthrough'  # Keep the other numerical columns unchanged\n",
    "    )\n",
    "\n",
    "print(f\"Splits guardados: {len(splits)}\")\n",
    "first_split = splits[0]\n",
    "print(f\"Treino: {first_split['X_train'].shape}, Teste: {first_split['X_test'].shape}\")\n",
    "for i, split in enumerate(splits):\n",
    "    X_test = split[\"X_test\"]  # Extract test set features\n",
    "    missing_values = X_test.isna().sum().sum()  # Count total NaN values\n",
    "\n",
    "    if missing_values > 0:\n",
    "        print(f\"⚠️ Split {i+1}: {missing_values} missing values found in the test set.\")\n",
    "    else:\n",
    "        print(f\"✅ Split {i+1}: No missing values in the test set.\")\n",
    "\n",
    "print(f\"Treino: {first_split['X_train'].shape}, Teste: {first_split['X_test'].shape}\")\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=4)\n",
    "for split in splits:\n",
    "\n",
    "    X_train_inputed = pd.DataFrame(knn_imputer.fit_transform(split[\"X_train\"]), \n",
    "                                   columns=split[\"X_train\"].columns)\n",
    "    X_test_inputed = pd.DataFrame(knn_imputer.transform(split[\"X_test\"]), \n",
    "                              columns=split[\"X_test\"].columns)\n",
    "    \n",
    "    print(\"Depois da inputação:\")\n",
    "    print(f\"Valores faltantes no treino - split:\", X_train_inputed.isna().sum().sum())\n",
    "\n",
    "    for col in split[\"X_train\"].columns:\n",
    "        if col not in one_hotted + cat_col_minus_oh:\n",
    "            original_values = split[\"X_train\"][col].dropna()\n",
    "            inputed_values = X_train_inputed[col]\n",
    "\n",
    "            ks_stat, p_value = ks_2samp(original_values, inputed_values)\n",
    "\n",
    "            if p_value < 0.05: print(f\"Variável {col}  NÃO tem distribuições similares (failed null-hypothesis) - {p_value}\")\n",
    "            #else: print(f\"Variável {col} NÃO tem distribuições similares (proved null-hypothesis) - {p_value}\")\n",
    "\n",
    "    #chi-quadrado para testar variação entre categóricas\n",
    "    for col in one_hotted + cat_col_minus_oh:\n",
    "        original_counts = split[\"X_train\"][col].value_counts()\n",
    "        inputed_counts = X_train_inputed[col].round().astype(int).value_counts()\n",
    "\n",
    "        original_counts= original_counts.reindex(inputed_counts.index, fill_value=0)\n",
    "        inputed_counts= inputed_counts.reindex(original_counts.index, fill_value=0)\n",
    "\n",
    "        chi2, p_value, _, _, = chi2_contingency([original_counts, inputed_counts])\n",
    "\n",
    "        if p_value < 0.05: print(f\"Variável {col} NÃO AFETADA pela inputação (failed null-hypothesis) - {p_value}\")\n",
    "        #else: print(f\"Variável {col} AFETADA pela inputação (proved null-hypothesis) - {p_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
