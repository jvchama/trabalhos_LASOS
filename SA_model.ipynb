{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from statsmodels.formula.api import ols\n",
    "import sklearn as sk \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essa célula serve pra expandir as colunas/linhas do df.head(), assim consigo ver todo o split e entender melhor como ele tá separando\n",
    "# pra voltar pra visualização padrão, só aplico o reset nele\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)  \n",
    "pd.set_option(\"display.max_columns\", None)  \n",
    "pd.set_option(\"display.width\", None) \n",
    "#pd.reset_option(\"display.max_rows\")\n",
    "#pd.reset_option(\"display.max_columns\")\n",
    "#pd.reset_option(\"display.width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Global_INT.xlsx\", sheet_name=\"Global-INT\")\n",
    "dicti = [\"continent_code\",\t\"country_code\",\t\"age\",\t\"sex\",\t\"gender_identity\",\t\"sexual_orientation\",\t\"height\",\t\"weight\",\t\"bmi\",\t\"marital_status\",\t\n",
    "         \"student_accommodation\",\t\"work\",\t\"income_reportada_corrigida\",\t\"income_categorias de renda pelo salário mínimo nacional_CLASSIFICAÇÃO\",\t\n",
    "         \"income_razão da renda pelo salário mínimo nacional\",\t\"income_grupos de referência pelo percentil_20,40,60,80,100\",\t\"score_food_smile\",\t\n",
    "         \"score_subs_smile\",\t\"score_Physical Activity_smile\",\t\"score_stress_smile\",\t\"score_social_smile\",\t\"score_sleep_smile\",\t\"score_envir_smile\",\t\n",
    "         \"score_total_smile\",\t\"sedentary_behavior\",\t'sedentary_2',\t\"gad7_score\",\t\"gad7_severidade de sintomas\"]\n",
    "\n",
    "cat_var = [\"continent_code\", \"country_code\", \"sex\",\t\"gender_identity\",\t\"sexual_orientation\", \"marital_status\",\t\n",
    "           \"student_accommodation\",\t\"work\", \"income_grupos de referência pelo percentil_20,40,60,80,100\", \"gad7_severidade de sintomas\",\n",
    "           \"sedentary_behavior\",\t'sedentary_2',]\n",
    "\n",
    "num_var = [\"age\", \"bmi\", \"score_food_smile\", \"score_subs_smile\",\t\"score_Physical Activity_smile\",\t\"score_stress_smile\",\t\n",
    "           \"score_social_smile\",\t\"score_sleep_smile\",\t\"score_envir_smile\",\t\n",
    "           \"score_total_smile\", \"gad7_score\"]\n",
    "\n",
    "df.drop(columns=[\"height\", \"weight\", \"income_reportada_corrigida\",\t\n",
    "                 \"income_categorias de renda pelo salário mínimo nacional_CLASSIFICAÇÃO\",\t\n",
    "                 \"income_razão da renda pelo salário mínimo nacional\",\"income_reportada_corrigida\",\t\n",
    "                 \"income_categorias de renda pelo salário mínimo nacional_CLASSIFICAÇÃO\"],\n",
    "                 inplace = True)\n",
    "\n",
    "for column in cat_var: \n",
    "    if not column == \"gad7_severidade de sintomas\": df[column] = df[column]-1\n",
    "\n",
    "df.to_excel(\"Global_INT_15_02.xlsx\", index = False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando os continentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#       separei os continentes em planilhas diferentes porque acho que assim conseguiria compartimentalizar\n",
    "#       o código de forma melhor. também estou mais acostumado a trabalhar com as planilhas assim, já que\n",
    "#       foi dessa forma que as utilizei na Fase 1\n",
    "df = pd.read_excel(\"Global_INT_15_02.xlsx\")\n",
    "\n",
    "print(\"Columns found:\", df.columns.tolist())\n",
    "print(\"Data preview:\\n\", df.head())\n",
    "\n",
    "if \"continent_code\" not in df.columns:\n",
    "    raise ValueError(\"The column 'continent_code' was not found in the file. Please check the header name.\")\n",
    "\n",
    "try:\n",
    "    df[\"continent_code\"] = df[\"continent_code\"].astype(int)\n",
    "except Exception as e:\n",
    "    print(\"Could not convert 'continent_code' to int:\", e)\n",
    "\n",
    "continent_mapping = {\n",
    "    0: \"América do Sul\",\n",
    "    1: \"América do Norte\",\n",
    "    2: \"Oceania\",\n",
    "    3: \"Ásia\",\n",
    "    4: \"África\",\n",
    "    5: \"Europa\"\n",
    "}\n",
    "\n",
    "for code, continent in continent_mapping.items():\n",
    "    subset = df[df[\"continent_code\"] == code]\n",
    "    output_filename = f\"{continent}_MODEL.xlsx\"\n",
    "    subset.to_excel(output_filename, index=False)\n",
    "    print(f\"Created {output_filename} with {len(subset)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição das variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#AMÉRICA DO SUL\n",
    "\n",
    "data = pd.read_excel(\"América do Sul_MODEL.xlsx\", sheet_name='Sheet1')\n",
    "\n",
    "cat_col_minus_oh = [\"sex\", \"student_accommodation\", \"work\", \n",
    "                    \"income_grupos de referência pelo percentil_20,40,60,80,100\",\n",
    "                    \"sedentary_behavior\", 'sedentary_2']\n",
    "\n",
    "one_hotted = [\"marital_status\", \"gender_identity\", \"sexual_orientation\", \"country_code\"]                \n",
    "target_col = \"gad7_severidade de sintomas\"\n",
    "\n",
    "data = data.drop(columns=[\"continent_code\", \"gad7_score\", \"score_total_smile\"])\n",
    "\n",
    "X = data.drop(columns=[target_col])\n",
    "Y = data[target_col]\n",
    "\n",
    "missing_data_idx = X[X.isna().any(axis=1)].index.to_numpy()\n",
    "complete_data_idx = X.dropna().index.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(\"América do Suil_MODEL_treated.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratando os splits - separando, codificando e escalonando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: (6167, 42), Teste: (1210, 42)\n"
     ]
    }
   ],
   "source": [
    "# separando os splits treino/teste (80/20)\n",
    "shuffle_split = ShuffleSplit(n_splits=5, test_size=0.2, random_state=50)\n",
    "splits = []\n",
    "\n",
    "one_hotted = [\"marital_status\", \"gender_identity\", \"sexual_orientation\", \"country_code\", 'sex', 'student_accommodation', 'work']  \n",
    "\n",
    "#cat_noh = ['sex', 'student_accommodation', 'work', 'sedentary_behavior','sedentary_2','income_grupos de referência pelo percentil_20,40,60,80,100']\n",
    "\n",
    "num = ['age', 'bmi', 'score_food_smile', 'score_subs_smile', 'score_Physical Activity_smile',\n",
    "       'score_stress_smile', 'score_social_smile', 'score_sleep_smile',\n",
    "       'score_envir_smile',  'sedentary_behavior','sedentary_2',\n",
    "       'income_grupos de referência pelo percentil_20,40,60,80,100']\n",
    "\n",
    "for train_pos, test_pos in shuffle_split.split(complete_data_idx):\n",
    "    train_idx = complete_data_idx[train_pos]\n",
    "    test_idx = complete_data_idx[test_pos]\n",
    "\n",
    "    train_idx = pd.Index(train_idx).union(missing_data_idx)  \n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "\n",
    "    # One-hot encoding e StandardScaler\n",
    "    onehot_transformer = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    #scaler_transformer = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_transformer = StandardScaler()\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"onehot\", onehot_transformer, one_hotted),\n",
    "            (\"scaler\", scaler_transformer, num),\n",
    "        ],\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    #  codficando e escalonando\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "    \n",
    "    #nomeando as variáveis nos splits - melhor visualização\n",
    "    onehot_names = preprocessor.named_transformers_[\"onehot\"].get_feature_names_out(one_hotted)\n",
    "    scaled_names = num\n",
    "\n",
    "    remaining = [col for col in X_train.columns if col not in one_hotted + num]\n",
    "\n",
    "    all_feature_names = np.concatenate([onehot_names, scaled_names, remaining])\n",
    "\n",
    "    X_train = pd.DataFrame(X_train_transformed, index=X_train.index, columns=all_feature_names)\n",
    "    X_test = pd.DataFrame(X_test_transformed, index=X_test.index, columns=all_feature_names)\n",
    "\n",
    "    # guardando splits com os dados transformados\n",
    "    splits.append({\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"Y_train\": Y_train,\n",
    "        \"Y_test\": Y_test\n",
    "    })\n",
    "\n",
    "inputed_splits = []\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "for split in splits: #iterando pelos splits\n",
    "    \n",
    "    X_train_inputed = pd.DataFrame(knn_imputer.fit_transform(split[\"X_train\"]), \n",
    "                                   columns=split[\"X_train\"].columns)\n",
    "    inputed_splits.append(X_train_inputed)\n",
    "# debugging: check do \"transformer\"\n",
    "print(f\"Treino: {splits[0]['X_train'].shape}, Teste: {splits[0]['X_test'].shape}\")\n",
    "\n",
    "#for split in splits:\n",
    "#    count = split[\"X_train\"][\"age\"].n\n",
    "#        print(split[\"X_train\"][\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[3]['X_test'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[3]['X_train'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando a imputação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#após separados, escalonados e codificados, aplicar inputação de dados faltantes\n",
    "#método k-Nearest Neighbour - nota: tentei aumentar e diminuir o n de vizinhos; pouca alteração\n",
    "knn_imputer = KNNImputer(n_neighbors=1)\n",
    "for split in splits:\n",
    "\n",
    "    X_train_imputed = pd.DataFrame(\n",
    "        knn_imputer.fit_transform(split[\"X_train\"]),\n",
    "        columns=split[\"X_train\"].columns,\n",
    "        index=split[\"X_train\"].index\n",
    "    )\n",
    "\n",
    "    split[\"X_train_imputed\"] = X_train_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[0][\"X_train_imputed\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[1][\"X_train_imputed\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testeando a similaridade dos dados inputados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = ['marital_status_0.0', 'marital_status_1.0', 'marital_status_2.0',\n",
    "       'marital_status_3.0', 'marital_status_4.0', 'marital_status_nan',\n",
    "       'gender_identity_0.0', 'gender_identity_1.0', 'gender_identity_nan',\n",
    "       'sexual_orientation_0.0', 'sexual_orientation_1.0',\n",
    "       'sexual_orientation_2.0', 'sexual_orientation_3.0',\n",
    "       'sexual_orientation_4.0', 'sexual_orientation_nan', 'country_code_0',\n",
    "       'country_code_1', 'country_code_2', 'country_code_3', 'country_code_4',\n",
    "       'country_code_5', \"sex_0.0\",\t\"sex_1.0\",\t\"sex_nan\",\t\"student_accommodation_0.0\",\t\n",
    "       \"student_accommodation_1.0\",\t\"student_accommodation_nan\",\t\"work_0.0\",\t\"work_1.0\",\t\"work_nan\"] \n",
    "\n",
    "ks = ['age', 'bmi', 'income_grupos de referência pelo percentil_20,40,60,80,100', \n",
    "      'score_food_smile', 'score_subs_smile', 'score_Physical Activity_smile',\n",
    "      'score_stress_smile', 'score_social_smile', 'score_sleep_smile',\n",
    "      'score_envir_smile', 'sedentary_behavior','sedentary_2']\n",
    "\n",
    "for i, split in enumerate(splits):\n",
    "\n",
    "    X_train_imputed = splits[i][\"X_train_imputed\"]\n",
    "\n",
    "    #testando a similaridade dos dados inputados vs originais\n",
    "    for col in split[\"X_train\"].columns:\n",
    "        #teste KS para variáveis numéricas\n",
    "        if col in ks:\n",
    "            original_values = split[\"X_train\"][col].dropna()\n",
    "            imputed_values = X_train_imputed[col]\n",
    "\n",
    "            ks_stat, p_value = ks_2samp(original_values, imputed_values)\n",
    "\n",
    "            if p_value < 0.05: print(f\"Variável {col}  NÃO tem distribuições similares (failed null-hypothesis) - {p_value}\")\n",
    "            #else: print(f\"Variável {col} NÃO tem distribuições similares (proved null-hypothesis) - {p_value}\")\n",
    "\n",
    "        #chi-quadrado para testar variação entre categóricas\n",
    "        elif col in c2:\n",
    "            original_counts = split[\"X_train\"][col].value_counts()\n",
    "            imputed_counts = X_train_imputed[col].round().astype(int).value_counts()\n",
    "\n",
    "            original_counts= original_counts.reindex(imputed_counts.index, fill_value=0)\n",
    "            imputed_counts= imputed_counts.reindex(original_counts.index, fill_value=0)\n",
    "\n",
    "            chi2, p_value, _, _ = chi2_contingency([original_counts, imputed_counts])\n",
    "\n",
    "            if p_value < 0.05: print(f\"Variável {col} AFETADA pela inputação (failed null-hypothesis) - {p_value}\")\n",
    "            #else: print(f\"Variável {col} AFETADA pela inputação (proved null-hypothesis) - {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse output, tem um erro que não sei explicar o que é exatamente; pesquisei e parece ser só uma \"mudança na forma de cálculo\" do teste KS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
